{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q py7zr\n",
    "%pip install -q xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas\n",
    "import py7zr\n",
    "import glob\n",
    "import xmltodict\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\teichx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\teichx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from unicodedata import normalize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_accent_diacritic(text: str):\n",
    "  return normalize('NFKD', text)\n",
    "    \n",
    "def remove_special_char(text: str):\n",
    "  def internal():\n",
    "    for character in text:\n",
    "      if character.isalnum() or character.isspace():\n",
    "        yield character\n",
    "  return ''.join(internal())\n",
    "\n",
    "def remove_stop_words(text: str):\n",
    "  words = word_tokenize(text)\n",
    "  return list(filter(lambda x: x not in stop_words, words))\n",
    "\n",
    "def lemmatize_text(text: str):\n",
    "  return ' '.join(map(word_net_lemmatizer.lemmatize, text))\n",
    "\n",
    "steps = [\n",
    "  str.lower,\n",
    "  remove_accent_diacritic,\n",
    "  remove_special_char,\n",
    "  remove_stop_words,\n",
    "  lemmatize_text,\n",
    "]\n",
    "\n",
    "def process_description(text):\n",
    "  return reduce(lambda result, func: func(result), steps, text)\n",
    "\n",
    "def pre_processing(df: pandas.DataFrame):\n",
    "  total_rows = len(df)\n",
    "\n",
    "  df.drop_duplicates(inplace=True)\n",
    "  # Remove the lines where the quantity purchased has decimal places, which indicates that the product was not manufactured in an industry, but is sold in bulk.\n",
    "  df = df[df['quantity'] % 1 == 0]\n",
    "  \n",
    "  df.info() \n",
    "\n",
    "  unique_rows = len(df)\n",
    "  removed_rows = total_rows - unique_rows\n",
    "  print(f'Total rows: {total_rows}')\n",
    "  print(f'Unique rows: {unique_rows}')\n",
    "  print(f'Removed rows: {removed_rows}')\n",
    "  print(f'Removed rows percentage: {round(removed_rows / total_rows * 100, 2)}%')\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cache combined data\n"
     ]
    }
   ],
   "source": [
    "def get_class_items():\n",
    "  folder_path = '../data/raw'\n",
    "  processed_files = 0\n",
    "  for file_name in os.listdir(folder_path):\n",
    "    if not file_name.endswith('.zip'):\n",
    "      continue\n",
    "    zip_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "      file_items_name = f'{file_name.removesuffix('.zip')}_NotaFiscalItem.csv'\n",
    "      with zip_ref.open(file_items_name) as file:\n",
    "        file.seek(0)\n",
    "        data_frame = pandas.read_csv(file, delimiter=';', encoding='latin1')\n",
    "        data_frame.rename(columns={\n",
    "          'CHAVE DE ACESSO': 'access_key',\n",
    "          'DATA EMISSÃO': 'emission_date',\n",
    "          'CPF/CNPJ Emitente': 'emission_owner',\n",
    "          'NÚMERO PRODUTO': 'product_index',\n",
    "          'DESCRIÇÃO DO PRODUTO/SERVIÇO': 'description',\n",
    "          'CÓDIGO NCM/SH': 'ncm',\n",
    "          'NCM/SH (TIPO DE PRODUTO)': 'ncm_description',\n",
    "          'CFOP': 'cfop',\n",
    "          'QUANTIDADE': 'quantity',\n",
    "          'UNIDADE': 'unit_kind',\n",
    "          'VALOR UNITÁRIO': 'unitary_value',\n",
    "          'VALOR TOTAL': 'total_value',\n",
    "        }, inplace=True)\n",
    "        selected_fields = data_frame \\\n",
    "          .loc[:, [\n",
    "            'access_key',\n",
    "            'emission_date',\n",
    "            'emission_owner',\n",
    "            'product_index',\n",
    "            'description',\n",
    "            'ncm',\n",
    "            'ncm_description',\n",
    "            'cfop',\n",
    "            'quantity',\n",
    "            'unit_kind',\n",
    "            'unitary_value',\n",
    "            'total_value',\n",
    "          ]]\n",
    "        selected_fields['quantity'] = selected_fields['quantity'].str.replace(',', '.')\n",
    "        selected_fields['total_value'] = selected_fields['total_value'].str.replace(',', '.')\n",
    "        selected_fields['unitary_value'] = selected_fields['unitary_value'].str.replace(',', '.')\n",
    "        selected_fields['processed_description'] = selected_fields['description'].apply(process_description)\n",
    "\n",
    "        yield selected_fields \\\n",
    "          .astype({\n",
    "            'access_key': 'str',\n",
    "            'emission_date': 'datetime64[ms]',\n",
    "            'emission_owner': 'str',\n",
    "            'product_index': 'int32',\n",
    "            'description': 'str',\n",
    "            'processed_description': 'str',\n",
    "            'ncm': 'str',\n",
    "            'ncm_description': 'str',\n",
    "            'cfop': 'str',\n",
    "            'quantity': 'float64',\n",
    "            'unit_kind': 'str',\n",
    "            'unitary_value': 'float64',\n",
    "            'total_value': 'float64',\n",
    "          })\n",
    "        processed_files += 1\n",
    "        print(f'Processed files {processed_files}')\n",
    "\n",
    "\n",
    "def get_class_combined():\n",
    "  combined_path = '../data/combined.parquet.br'\n",
    "  if os.path.exists(combined_path):\n",
    "    print('Reading cache combined data')\n",
    "    return pandas.read_parquet(combined_path)\n",
    "  combined_data = pandas.concat(get_class_items(), ignore_index=True)\n",
    "  print('Start pre-processing')\n",
    "  processed_data = pre_processing(combined_data)\n",
    "  print('End pre-processing')\n",
    "  print('Start compression')\n",
    "  processed_data.to_parquet(combined_path, index=False, compression='brotli')\n",
    "  print('End compression')\n",
    "  return processed_data\n",
    "\n",
    "data_frame = get_class_combined()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = dict(map(lambda x: (str(x), {'unique': 0}), range(0, 100)))\n",
    "for table_item in data_frame['access_key'].unique():\n",
    "  table[table_item[0:2]]['unique'] += 1\n",
    "  \n",
    "table = dict(filter(lambda x: x[1]['unique'] > 0, table.items()))\n",
    "for key in table.keys():\n",
    "  current_state = data_frame.where(data_frame['access_key'].str.startswith(key))\n",
    "  total_value = current_state['total_value']\n",
    "  table[key] |= {\n",
    "    'total': total_value.count(),\n",
    "    'total_value': round(total_value.sum(), 2),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquired from 'https://servicodados.ibge.gov.br/api/v1/localidades/estados' 2024-10-27T10:24:00-03:00\n",
    "with open('../data/br_states.json', encoding='utf8') as file:\n",
    "  states_json = json.loads(file.read())\n",
    "  state_codes = dict(map(lambda x: (str(x['id']), x), states_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    UF                State  Unique invoices  Product rows  Total amount\n",
      "0   SP            São Paulo           958658       4347276  9.930479e+10\n",
      "1   RJ       Rio de Janeiro           753427       2428270  4.477481e+10\n",
      "2   RS    Rio Grande do Sul           402972       1871396  5.318799e+09\n",
      "3   MG         Minas Gerais           331109       1222405  1.789578e+10\n",
      "4   DF     Distrito Federal           376974       1036455  1.515608e+10\n",
      "5   PR               Paraná           257581        920613  8.389014e+09\n",
      "6   SC       Santa Catarina           213615        553848  5.367721e+09\n",
      "7   PE           Pernambuco           141205        477757  6.922940e+09\n",
      "8   MS   Mato Grosso do Sul            88916        414219  9.358505e+08\n",
      "9   BA                Bahia           107064        413975  1.460084e+09\n",
      "10  PA                 Pará            97942        391397  9.696880e+08\n",
      "11  AM             Amazonas            96810        384924  1.309225e+09\n",
      "12  RN  Rio Grande do Norte            94481        320825  4.157402e+08\n",
      "13  GO                Goiás           106960        304182  2.324010e+10\n",
      "14  CE                Ceará            80147        278661  6.374870e+08\n",
      "15  MA             Maranhão            62931        248508  3.587004e+08\n",
      "16  ES       Espírito Santo            84399        239569  3.409327e+09\n",
      "17  MT          Mato Grosso            51972        182676  4.522654e+08\n",
      "18  PI                Piauí            33547        163756  2.121944e+08\n",
      "19  PB              Paraíba            55306        160939  4.521437e+08\n",
      "20  RO             Rondônia            43898        142441  3.528114e+08\n",
      "21  RR              Roraima            25956        119747  5.966037e+08\n",
      "22  AC                 Acre            18229         77140  1.205716e+08\n",
      "23  AL              Alagoas            25626         75676  2.208108e+08\n",
      "24  AP                Amapá            16399         63338  8.350801e+07\n",
      "25  TO            Tocantins            17699         60476  1.195895e+08\n",
      "26  SE              Sergipe            17413         51446  1.630883e+08\n"
     ]
    }
   ],
   "source": [
    "meta_table_rows = list(table.items())\n",
    "meta_table_rows.sort(key=lambda x: x[1]['total'], reverse=True)\n",
    "meta_data_table = pandas.DataFrame(\n",
    "  data=map(lambda x: {\n",
    "    'UF': state_codes.get(x[0])['sigla'],\n",
    "    'State': state_codes.get(x[0])['nome'],\n",
    "    'Unique invoices': x[1]['unique'],\n",
    "    'Product rows': x[1]['total'],\n",
    "    'Total amount': x[1]['total_value'],\n",
    "  }, meta_table_rows), \n",
    ")\n",
    "print(meta_data_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cache combined data\n"
     ]
    }
   ],
   "source": [
    "def get_class_items():\n",
    "  folder_path = '../data/tce_rs'\n",
    "  processed_files = 0\n",
    "  for file_name in os.listdir(folder_path):\n",
    "    if not file_name.endswith('.xml.7z'):\n",
    "      continue\n",
    "    zip_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    with py7zr.SevenZipFile(zip_path, mode='r') as zip_ref:\n",
    "      zip_ref.extractall(folder_path)\n",
    "      \n",
    "  for invoice_xml in glob.glob(f'{folder_path}/**/*.xml'):\n",
    "    processed_files += 1\n",
    "    with open(invoice_xml, 'r', encoding='utf-8') as file:\n",
    "      data = xmltodict.parse(file.read())\n",
    "      products = data.get('nfeProc', {}).get('NFe', {}).get('infNFe', {}).get('det', [])\n",
    "      product_list = products \\\n",
    "        if isinstance(products, list) \\\n",
    "        else [products]\n",
    "      \n",
    "      if not product_list:\n",
    "        continue\n",
    "        \n",
    "      access_key = data['nfeProc']['protNFe']['infProt']['chNFe']\n",
    "      cnpj = data['nfeProc']['NFe']['infNFe']['emit']['CNPJ']\n",
    "      emission_date = pandas.to_datetime(data['nfeProc']['NFe']['infNFe']['ide']['dhEmi']) \\\n",
    "        .tz_convert(None)\n",
    "      for product in product_list:\n",
    "        ean = product['prod']['cEAN']\n",
    "        if not ean or ean == 'SEM GTIN':\n",
    "          continue\n",
    "        \n",
    "        description = product['prod']['xProd']\n",
    "        yield {\n",
    "          'access_key': access_key,\n",
    "          'emission_date': emission_date,\n",
    "          'emission_owner': cnpj,\n",
    "          'product_index': int(product['@nItem']),\n",
    "          'description': description,\n",
    "          'ean': ean,\n",
    "          'ncm': product['prod']['NCM'],\n",
    "          'cfop': product['prod']['CFOP'],\n",
    "          'quantity': float(product['prod']['qCom']),\n",
    "          'unit_kind': product['prod']['uCom'],\n",
    "          'unitary_value': float(product['prod']['vUnCom']),\n",
    "          'total_value': float(product['prod']['vProd']),\n",
    "          'processed_description': process_description(description),\n",
    "        }\n",
    "        \n",
    "    if processed_files % 100 == 0:\n",
    "      print(f'Processed files: {processed_files}')\n",
    "\n",
    "def get_class_combined():\n",
    "  class_path = '../data/class.parquet.br'\n",
    "  if os.path.exists(class_path):\n",
    "    print('Reading cache combined data')\n",
    "    return pandas.read_parquet(class_path)\n",
    "\n",
    "  class_data = pandas.DataFrame(get_class_items()) \\\n",
    "    .astype({\n",
    "      'access_key': 'str',\n",
    "      'emission_date': 'datetime64[ms]',\n",
    "      'emission_owner': 'str',\n",
    "      'product_index': 'int32',\n",
    "      'description': 'str',\n",
    "      'ncm': 'str',\n",
    "      'ean': 'str',\n",
    "      'cfop': 'str',\n",
    "      'quantity': 'float64',\n",
    "      'unit_kind': 'str',\n",
    "      'unitary_value': 'float64',\n",
    "      'total_value': 'float64',\n",
    "    })\n",
    "  print('Start pre-processing')\n",
    "  processed_data = pre_processing(class_data)\n",
    "  print('End pre-processing')\n",
    "  print('Start compression')\n",
    "  processed_data.to_parquet(class_path, index=False, compression='brotli')\n",
    "  print('End compression')\n",
    "  return processed_data\n",
    "\n",
    "class_data = get_class_combined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_dict = dict()\n",
    "for row in class_data.itertuples():\n",
    "  if not descriptions_dict.get(row.processed_description):\n",
    "    descriptions_dict[row.processed_description] = set()\n",
    "    \n",
    "  item = frozenset([\n",
    "    ('ncm', row.ncm),\n",
    "    ('ean', row.ean),\n",
    "  ])\n",
    "  descriptions_dict[row.processed_description].add(item)\n",
    "  \n",
    "for key, value in descriptions_dict.items():\n",
    "  descriptions_dict[key] = list(map(dict, value))\n",
    "  \n",
    "with open('../data/ean_list.json', 'w') as file:\n",
    "  file.write(json.dumps(descriptions_dict, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
